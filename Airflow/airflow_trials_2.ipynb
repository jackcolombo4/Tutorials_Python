{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height:1.2;\">\n",
    "\n",
    "<h1 style=\"color:darkturquoise; margin-bottom: 0.3em;\">Airflow tutorial 2 </h1>\n",
    "\n",
    "<p style=\"margin-top: 0.5em; margin-bottom: 1.5em;\"><strong> Data pipelines (work in progress..) </strong></p>\n",
    "\n",
    "<div style=\"line-height:1.4; margin-bottom: 1em;\">\n",
    "    <h3 style=\"color: lightblue; display: inline; margin-right: 0.5em;\">Keywords:</h3>\n",
    "    <span style=\"display: inline;\">DAG creation + @dag and @task decorators + DagBag + annotations + LoggingMixin + Executor + smtplib </span>\n",
    "</div>\n",
    "\n",
    "<div style=\"line-height:1.4; margin-top: 1em;\">\n",
    "    <h3 style=\"color: red; display: inline; margin-right: 0.5em;\">Notes:</h3>\n",
    "    <span style=\"display: inline;\">\n",
    "    Jupyter Notebooks must be converted to Python Script to work, since not designed for task scheduling and workflow management. <br>\n",
    "    All the dags \".py\" files need to be placed and place it in the Airflow's dags folder. => ~/airflow/dags (or the dir specified in the \"airflow.cfg\" config file) <br>\n",
    "    In Apache Airflow, it is possible to define multiple DAGs into the same Python file. It is not mandatory having separate file for each DAG. <br>\n",
    "    </span>\n",
    "</div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import random\n",
    "import pendulum\n",
    "from datetime import datetime, timedelta\n",
    "from airflow.notifications.basenotifier import BaseNotifier\n",
    "\n",
    "from airflow import DAG\n",
    "#OR \n",
    "#from airflow.models.dag import DAG\n",
    "# both import statements are equivalent and will have the same effect in your Airflow scripts.\n",
    "\n",
    "from airflow.models import DagBag\n",
    "from airflow.decorators import dag, task\n",
    "from airflow.utils.context import Context\n",
    "from airflow.utils.email import send_email\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.utils.log.logging_mixin import LoggingMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<h3 style=\"color: darkturquoise;\"> Recap: Python annotations </h3>\n",
    "<div style=\"margin-top: -8px;\">\n",
    "=> Basically, it is giving type to methods' parameters. <br>\n",
    "Annotations are used to enable postponed evaluation of type annotations, to allow the specification of the expected types of input and output values for the tasks. <br>\n",
    "In this is way it is possible to ensure that the type annotations specified in the function declarations will be preserved at runtime. \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: darkturquoise;\"> Recap: LoggingMixin </h3>\n",
    "<div style=\"margin-top: -8px;\">\n",
    "The LoggingMixin can be used to have have a logger configured with the class name. <br>\n",
    "It is often used to simplify logging in custom operators, sensors, hooks, or other Airflow components, to ensure that your custom Airflow components log information in a way that aligns with the Airflow framework's logging conventions.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<h2 style=\"color: darkturquoise;\"> Example #5 </h2>\n",
    "Simple data pipeline to perform Extract, Transform, and Load procedures in 3 tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "N.B.1\n",
    "The '@dag' and '@task' decorators part of the Airflow TaskFlow API and are specific to Apache Airflow.\n",
    "They provide a Pythonic and programmatic way to create and manage workflows in Airflow.\n",
    "They can be used to define and configure Directed Acyclic Graphs and individual tasks within them. \n",
    "These decorators are used to convert a regular Python function into a graph and a task, by adding metadata and configuration settings.\n",
    "\"\"\"\n",
    "@dag(\n",
    "    schedule=None,\n",
    "    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n",
    "    catchup=False,\n",
    "    tags=[\"example_5\"],\n",
    ")\n",
    "def function_5_create_data_pipeline_for_ETL():\n",
    "    @task()\n",
    "    def extract():\n",
    "        \"\"\" Simulate the extraction of data from a formatted a JSON object. \"\"\"\n",
    "        ##### Data strings with various types of content\n",
    "        data_string1 = json.dumps({f\"key_{i}\": f\"value_{i}\" for i in range(1, 11)})\n",
    "        data_ints2 = json.dumps({f\"int_key_{i}\": i * 100 for i in range(1, 11)})\n",
    "        data_values3 = json.dumps({f\"list_key_{i}\": [i, i * 2, i * 3] for i in range(1, 11)})\n",
    "        data_bools4 = json.dumps({f\"bool_key_{i}\": i % 2 == 0 for i in range(1, 11)})\n",
    "        data_nest5 = json.dumps({f\"nested_key_{i}\": {\"subkey\": i, \"subvalue\": f\"sub_{i}\"} for i in range(1, 11)})\n",
    "        ##### Parse the JSON formatted strings and convert them into Python dictionaries\n",
    "        ddict1 = json.loads(data_string1)\n",
    "        ddict2 = json.loads(data_ints2)\n",
    "        ddict3 = json.loads(data_values3)\n",
    "        ddict4 = json.loads(data_bools4)\n",
    "        ddict5 = json.loads(data_nest5)\n",
    "        # Combine all dictionaries into one\n",
    "        combined_data = {**ddict1, **ddict2, **ddict3, **ddict4, **ddict5}\n",
    "\n",
    "        return combined_data\n",
    "    ####################################################################################\n",
    "    @task(multiple_outputs=True)\n",
    "    def transform(combined_data_dict: dict):\n",
    "        \"\"\" Transform a collection of diverse data types by computing the total value of integer entries. \"\"\"\n",
    "        total_order_value = 0\n",
    "        ### Loop through the values and sum up only the integer values\n",
    "        for value in combined_data_dict.values():\n",
    "            if isinstance(value, int):\n",
    "                total_order_value += value\n",
    "\n",
    "        return {\"total_order_value\": total_order_value}\n",
    "    ####################################################################################\n",
    "    @task()\n",
    "    def load(total_order_value: float):\n",
    "        \"\"\" Simulate the load by printing the result of the Transform task, instead of saving it. \"\"\"\n",
    "        print(f\"Total order value is: {total_order_value:.2f}\")\n",
    "    \n",
    "    order_data = extract()\n",
    "    order_summary = transform(order_data)\n",
    "    load(order_summary[\"total_order_value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: darkturquoise;\"> Recap: Common parameters for the @dag decorator</h3>\n",
    "<div style=\"margin-top: -8px;\">\n",
    "    \n",
    "1. schedule (optional): Defines how often the DAG should run. This could be a cron expression, a datetime.timedelta object, or one of Airflows preset intervals like @daily.\n",
    "\n",
    "2. start_date (required): The start date for the DAG. This is a datetime.datetime object that marks the earliest date at which the DAG can start running.\n",
    "\n",
    "3. end_date (optional): The end date for the DAG. This is a datetime.datetime object that, if specified, prevents the DAG from executing after this date.\n",
    "\n",
    "4. tags (optional): A list of tags that can be used for filtering and organizing DAGs within the Airflow UI.\n",
    "\n",
    "5. catchup (optional, default True): If set to False, Airflow will only run the latest instance of the DAG and skip any missed intervals since the last run.\n",
    "\n",
    "6. max_active_runs (optional): This parameter limits the number of DAG runs that can be running concurrently.\n",
    "\n",
    "7. default_args (optional): A dictionary of default parameters to be applied to all operators in the DAG.\n",
    "\n",
    "8. description (optional): A string describing the DAG, which is visible in the Airflow UI.\n",
    "\n",
    "9. on_failure_callback (optional): A function that is called when a DAG fails. This can be used for custom error handling.\n",
    "\n",
    "10. on_success_callback (optional): Similar to on_failure_callback, but this function is called when the DAG succeeds.\n",
    "\n",
    "11. on_retry_callback (optional): A function that is called before retrying a failed DAG.\n",
    "\n",
    "12. max_active_tasks (optional): Sets the number of tasks that can run simultaneously for this DAG.\n",
    "\n",
    "13. dagrun_timeout (optional): Specifies the maximum runtime for a DAG run beyond which it will be marked as failed.\n",
    "\n",
    "14. doc_md (optional): Markdown text that will be displayed in the Airflow web UI about this DAG.\n",
    "\n",
    "15. orientation (optional): Controls the orientation of the DAG visualization in the Airflow UI. It can be either LR (left-to-right) or TB (top-to-bottom).\n",
    "\n",
    "16. render_template_as_native_obj (optional): When set to True, template fields will be rendered to their native Python types rather than strings.\n",
    "\n",
    "17. template_searchpath (optional): A list of folders (or paths) that Airflow should look in for loading templates.\n",
    "\n",
    "18. sla_miss_callback (optional): A function to call when an SLA (Service Level Agreement) miss occurs.\n",
    "\n",
    "19. params (optional): A dictionary of DAG-level parameters that are accessible from operators.\n",
    "\n",
    "20. access_control (optional): A dictionary defining the access control for the DAG, useful in multi-tenant environments.\n",
    "\n",
    "21. is_paused_upon_creation (optional): If set to True, the DAG will be in a paused state when created for the first time.\n",
    "\n",
    "These parameters provide extensive customization and control over the behavior and execution of DAGs in Airflow, making the @dag decorator a fundamental tool for defining workflows.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: darkturquoise;\"> Recap: Common parameters for the @task decorator </h3>\n",
    "<div style=\"margin-top: -8px;\">\n",
    "\n",
    "1. python_callable (optional): This is the Python function you are decorating. Generally, you dont need to explicitly specify this, as the decorator is applied directly to the function.\n",
    "\n",
    "2. multiple_outputs (optional, default False): If set to True, this allows the task to return a dictionary where each key will be used as an output XCom key. This is useful when your task returns multiple values that you want to pass to downstream tasks.\n",
    "\n",
    "3. task_id (optional): This is the identifier for the task within the DAG. If not specified, Airflow will automatically generate an ID based on the function name.\n",
    "\n",
    "4. executor_config (optional): This parameter can be used to provide executor-specific configuration settings, particularly useful in different execution environments like KubernetesExecutor.\n",
    "\n",
    "5. retry_delay (optional): Sets the delay between retry attempts. It accepts a datetime.timedelta value.\n",
    "\n",
    "6. retries (optional): Determines the number of retries that should be attempted in case of failure.\n",
    "\n",
    "7. trigger_rule (optional): Defines the rule by which the task gets triggered. The default value is \"all_success\", meaning the task will run when all its upstream tasks have succeeded.\n",
    "\n",
    "8. depends_on_past (optional, default False): If set to True, the task instance depends on the success of the task instance for the previous schedule period.\n",
    "\n",
    "9. provide_context (optional, default False): If set to True, Airflow will pass a set of keyword arguments that can be used in your function. This is mostly used in Airflow 1.x and is less relevant in Airflow 2.0 due to the introduction of XComArgs.\n",
    "\n",
    "10. params (optional): Allows passing a dictionary of parameters that can be accessed by the task.\n",
    "\n",
    "11. dag (optional): Specifies the DAG that the task belongs to. Typically, you dont need to set this if your task is defined within the context of a DAG.\n",
    "\n",
    "12. op_args and op_kwargs (optional): These are lists and dictionaries of positional and keyword arguments that will get unpacked and passed into your Python callable.\n",
    "\n",
    "13. on_failure_callback, on_success_callback, and on_retry_callback (optional): These parameters allow you to specify functions that should be called on task failure, success, or retry.\n",
    "\n",
    "14. start_date (optional): The start date of the task.\n",
    "\n",
    "15. end_date (optional): The end date of the task.\n",
    "\n",
    "16. doc_md (optional): Markdown text that will be displayed in the Airflow web UI about this task.\n",
    "\n",
    "17. timeout (optional): The maximum runtime of the task before it gets terminated.\n",
    "\n",
    "18. pool (optional): The pool parameter allows assigning a task to a specific pool of resources. Pools are a way to limit the execution parallelism on arbitrary sets of tasks. <br> \n",
    "They are used to constrain the number of running instances of a task across the DAGs. <br>\n",
    "Each pool can have a predefined number of slots (parallelism capacity). When a task is assigned to a pool, it will use one of these slots when running. <br>\n",
    "If no slots are available, the task will wait in a queued state until a slot is free.\n",
    "\n",
    "19. queue: The queue parameter is used when working with the CeleryExecutor in Airflow. <br> \n",
    "It allows specifying which queue (in the Celery task queue sense) the task should be sent to. <br> \n",
    "Useful for directing certain tasks to specific workers that might have different capabilities or resources. <br>\n",
    "Each queue can be consumed by a specific set of workers. <br>\n",
    "By assigning tasks to different queues, it is possible to control which worker processes which task. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DAG: function_5_create_data_pipeline_for_ETL>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" no need to create the dag variable as in previous examples. \"\"\"\n",
    "function_5_create_data_pipeline_for_ETL()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: darkturquoise;\"> Example #6 </h2>\n",
    "Data pipeline with more complex procedures for extraction, transforming, and loading.\n",
    "Add parameters to the decorators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_args = {\n",
    "    \"owner\": \"Jack4\", \n",
    "    \"depends_on_past\": True,                                # Wait for the success of the previous run of the same task\n",
    "    \"email_on_failure\": False,                              # Send an email if a task fails\n",
    "    \"email_on_retry\": False,                                # Send an email on task retry\n",
    "    \"email\": [\"example_email@example.com\"],                 # Where send notifications\n",
    "    \"retries\": 1,                                           # Retry the specified number of times, if a task fails\n",
    "    \"retry_delay\": timedelta(minutes=5),                    # Delay between retries\n",
    "    \"catchup\": False,                                       # If False, run the latest instance when it falls behind schedule\n",
    "    \"start_date\": pendulum.datetime(2021, 1, 1, tz=\"UTC\"),  # Start date of workflow\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: darkturquoise;\"> Recap: LoggingMixin </h3>\n",
    "<div style=\"margin-top: -8px;\">\n",
    "The LoggingMixin can be used to have have a logger configured with the class name. <br>\n",
    "It is often used to simplify logging in custom operators, sensors, hooks, or other Airflow components, <br> \n",
    "to ensure that everything is logged in a way that aligns with the Airflow framework's logging conventions.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback_for_failure(context):\n",
    "    \"\"\" Log errors in case of failures during task execution, given the context of the task. \"\"\"\n",
    "    task_instance = context['task_instance']\n",
    "    dag_id = task_instance.dag_id\n",
    "    task_id = task_instance.task_id\n",
    "    execution_date = context['execution_date']\n",
    "    exception = context['exception']\n",
    "    \n",
    "    ### Log info about failures\n",
    "    logger = LoggingMixin().log\n",
    "    logger.error(f\"Failure detected in task '{task_id}' of DAG '{dag_id}' on {execution_date}\")\n",
    "    logger.error(f\"Exception: {exception}\")\n",
    "    ### Send an email notification\n",
    "    subject = f\"Airflow Task Failure: {dag_id}.{task_id}\"\n",
    "    body = f\"Task '{task_id}' in DAG '{dag_id}' failed on {execution_date}\\nException: {exception}\"\n",
    "    send_email(to=\"your_email@example.com\", subject=subject, html_content=body)\n",
    "\n",
    "def callback_for_success(context: Context):\n",
    "    task_instance = context['task_instance']\n",
    "    dag_id = task_instance.dag_id\n",
    "    task_id = task_instance.task_id\n",
    "    execution_date = context['execution_date']\n",
    "    print(f\"Task {task_id} in DAG {dag_id} succeeded on {execution_date}\")\n",
    "\n",
    "def callback_for_retry(context: Context):\n",
    "    task_instance = context['task_instance']\n",
    "    dag_id = task_instance.dag_id\n",
    "    task_id = task_instance.task_id\n",
    "    execution_date = context['execution_date']\n",
    "    print(f\"Task {task_id} in DAG {dag_id} will retry on {execution_date}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DAG: function_6_create_another_pipeline_for_ETL>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dag(\n",
    "    schedule=None,\n",
    "    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n",
    "    end_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n",
    "    catchup=False,\n",
    "    tags=[\"example_5\"],\n",
    "    default_args=default_args,\n",
    "    description=\"Example DAG number 6\",\n",
    "    max_active_runs=3,\n",
    "    dagrun_timeout=timedelta(hours=2),\n",
    "    doc_md=\"\"\"Here is the DAG documentation of our example DAG in Markdown format.\"\"\",\n",
    "    max_active_tasks=10,\n",
    "    on_failure_callback=None,\n",
    "    on_success_callback=None,\n",
    "    sla_miss_callback=None,\n",
    "    params={\"example_param\": \"param_value\"},\n",
    "    access_control={\"role1\": {\"can_read\", \"can_edit\"}},\n",
    "    is_paused_upon_creation=False,\n",
    "    orientation=\"LR\"\n",
    ")\n",
    "def function_6_create_another_pipeline_for_ETL():\n",
    "    @task(\n",
    "        retries=3,\n",
    "        retry_delay=timedelta(minutes=5),\n",
    "        timeout=300,\n",
    "        on_failure_callback=callback_for_failure,\n",
    "        on_success_callback=callback_for_success,\n",
    "        on_retry_callback=callback_for_retry,\n",
    "        # Specify the executor_config parameter with Kubernetes-specific clearly does not make sense and will have no effect outside of a Kubernetes container.\n",
    "        #executor_config={\"KubernetesExecutor\": {\"image\": \"~/my-customimage:latest\"}}, \n",
    "        pool=\"default_pool\",\n",
    "        queue=\"default_queue\",\n",
    "        trigger_rule=\"all_success\"\n",
    "    )\n",
    "    def extract():\n",
    "        \"\"\" Simulate data fetching from an external source \"\"\"\n",
    "        data_string = json.dumps({\n",
    "            \"1001\": random.uniform(200, 400), \n",
    "            \"1002\": random.uniform(400, 600), \n",
    "            \"1003\": random.uniform(500, 700)\n",
    "        })\n",
    "        order_data_dict = json.loads(data_string)\n",
    "        filtered_data = {k: v for k, v in order_data_dict.items() if v > 300}\n",
    "        return filtered_data    \n",
    "    \n",
    "    @task(\n",
    "        multiple_outputs=True,\n",
    "        retries=2,\n",
    "        retry_delay=timedelta(minutes=10),\n",
    "        timeout=600,\n",
    "        on_failure_callback=callback_for_failure,\n",
    "        on_success_callback=callback_for_success,\n",
    "        on_retry_callback=callback_for_retry,\n",
    "        pool=\"high_priority_pool\",\n",
    "        queue=\"high_priority_queue\",\n",
    "        trigger_rule=\"one_success\"\n",
    "    )\n",
    "    def transform(my_data_extracted: dict):\n",
    "        total_order_value = 0\n",
    "        order_count = len(my_data_extracted)\n",
    "\n",
    "        max_order_value = -float('inf')\n",
    "        min_order_value = float('inf')\n",
    "\n",
    "        for value in my_data_extracted.values():\n",
    "            total_order_value += value\n",
    "            max_order_value = max(max_order_value, value)\n",
    "            min_order_value = min(min_order_value, value)\n",
    "\n",
    "        if order_count == 0:\n",
    "            average_order_value = 0\n",
    "            max_order_value = 0\n",
    "            min_order_value = 0\n",
    "        else:\n",
    "            average_order_value = total_order_value / order_count\n",
    "\n",
    "        ## Handle errors in case of unexpected data formats\n",
    "        if total_order_value < 0:\n",
    "            raise ValueError(\"Total order value cannot be negative\")\n",
    "        # Return a dictionary\n",
    "        return {\n",
    "            \"total_order_value\": total_order_value,\n",
    "            \"average_order_value\": average_order_value,\n",
    "            \"max_order_value\": max_order_value,\n",
    "            \"min_order_value\": min_order_value\n",
    "        }\n",
    "    @task(\n",
    "        retries=1,\n",
    "        retry_delay=timedelta(minutes=15),\n",
    "        timeout=120,\n",
    "        email_on_failure=True,\n",
    "        email=[\"example_mail@emailprovider.com\"],\n",
    "        pool=\"special_pool_6\",\n",
    "        queue=\"special_queue_6\",\n",
    "        trigger_rule=\"all_failed\",\n",
    "        weight_rule=\"downstream\",\n",
    "        end_date=datetime(2024, 1, 1)\n",
    "    )\n",
    "    def load(final_dict: dict):\n",
    "        \"\"\" Simulate the loading of transformed data to a target destination, by printing the result. \"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        for key, value in final_dict.items():\n",
    "            print(f\"Key: {key}, Value: {value}\")\n",
    "\n",
    "        print(\"Data loaded successfully.\")\n",
    "        return \"Load completed\"\n",
    "\n",
    "\n",
    "task_6_etl = function_6_create_another_pipeline_for_ETL() \n",
    "task_6_etl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: darkturquoise;\"> Example #7 </h2>\n",
    "Retrieve info about a DAG with DagBag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: darkturquoise;\"> Recap: Kubernetes </h3>\n",
    "<div style=\"margin-top: -8px;\">\n",
    "\n",
    "Dagbag does not clearly requires the Kubernetes Executor in Apache Airflow. <br>\n",
    "However it is requested in this case since it include imports or logic that require Kubernetes-related libraries. <br>\n",
    "=> Kubernetes Executor is an executor for running Airflow tasks in Kubernetes.  <br>\n",
    "Kubernetes is an open-source platform designed to automate managing containers => deploying, scaling, and operating application. <br>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2024-01-18T12:49:39.134+0100\u001b[0m] {\u001b[34mdagbag.py:\u001b[0m538} INFO\u001b[0m - Filling up the DagBag from /home/notto4/airflow/dags\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAG ID: function_5_create_data_pipeline_for_ETL\n",
      "Description: None\n",
      "Schedule Interval: None\n",
      "Start Date: Not specified\n",
      "End Date: Not specified\n",
      "Number of Tasks: 3\n",
      "Tags: ['example_5']\n",
      "Last Run: None\n"
     ]
    }
   ],
   "source": [
    "def print_dag_summary(dag_id):\n",
    "    \"\"\" Access and return some relevant properties of the given DAG.\\\\\n",
    "    The info are taken from Airflow not from the code above, therefore the DAG need to be created in a python file in the airflow/dags dir.\\\\\n",
    "    To create a dag it is necessary to indicate its owner in the 'default_args' dict that need to be added as a parameter of the @dag decorator.\n",
    "    \"\"\"\n",
    "    dag_bag = DagBag()\n",
    "    dag = dag_bag.get_dag(dag_id)\n",
    "\n",
    "    if dag is None:\n",
    "        print(f\"No DAG found with id: {dag_id}\")\n",
    "        return\n",
    "\n",
    "    print(f\"DAG ID: {dag.dag_id}\")\n",
    "    print(f\"Description: {dag.description}\")\n",
    "    print(f\"Schedule Interval: {dag.schedule_interval}\")\n",
    "    print(f\"Start Date: {dag.default_args.get('start_date', 'Not specified')}\")\n",
    "    print(f\"End Date: {dag.default_args.get('end_date', 'Not specified')}\")\n",
    "    print(f\"Number of Tasks: {len(dag.tasks)}\")\n",
    "    print(f\"Tags: {dag.tags}\")\n",
    "    print(f\"Last Run: {dag.get_last_dagrun()}\")\n",
    "\n",
    "\"\"\" When the DAG is created with the decorator without defining a specific dag_id in the variable definition, \n",
    "it is necessary the name of the dag method to identify the DAG. \n",
    "\"\"\"\n",
    "print_dag_summary('function_5_create_data_pipeline_for_ETL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: darkturquoise;\"> Notes: Executors </h3>\n",
    "<div style=\"margin-top: -8px;\">\n",
    "If no executor is defined as parameter, Airflow chose the SequentialExecutor as default, for creating simple, single-process workflows. <br>\n",
    "The SequentialExecutor runs one task at a time in a single process. It does not run tasks in parallel. <br>\n",
    "When dealing with more complex scenarios, typically configure Airflow to use an executors are typically configured to support parallelism.  <br>\n",
    "<div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SequentialExecutor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **LocalExecutor** can be instead used to enable parallelism in a simple manner. <br>\n",
    "Write the following lines into the 'airflow.cfg' file, then restart Airflow Services\n",
    "\n",
    "[core] <br>\n",
    "executor = LocalExecutor <br>\n",
    "[core] <br>\n",
    "sql_alchemy_conn = mysql+pymysql://username:password@localhost/airflow <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: darkturquoise;\"> Example #8 </h2>\n",
    "Create a notifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit \"airflow.cfg\" to include your SMTP settings: <br>\n",
    "\n",
    "[email] <br>\n",
    "email_backend = airflow.utils.email.send_email_smtp <br>\n",
    "\n",
    "[smtp]  <br>\n",
    "smtp_host = smtp.example.com <br>\n",
    "smtp_starttls = True <br>\n",
    "smtp_ssl = False <br>\n",
    "smtp_user = email_tmp_ok@guerillmail.info <br>\n",
    "smtp_password = your_password <br>\n",
    "smtp_port = 587 <br>\n",
    "smtp_mail_from = email_tmp_ok@guerillmail.info <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "def send_my_message(subject, message, to_email):\n",
    "    #### Create the SMTP server configuration\n",
    "    smtp_host = 'smtp.example.com'\n",
    "    smtp_user = 'email_tmp_ok@guerillmail.info'\n",
    "    smtp_password = 'tmpTestEmailPwd'\n",
    "    from_email = 'airflow@example.com'\n",
    "\n",
    "    #### Create the email content\n",
    "    msg = MIMEMultipart()\n",
    "    msg['From'] = from_email\n",
    "    msg['To'] = to_email\n",
    "    msg['Subject'] = subject\n",
    "    msg.attach(MIMEText(message, 'plain'))\n",
    "\n",
    "    ########## Send the email\n",
    "    try:\n",
    "        server = smtplib.SMTP(smtp_host, 587)\n",
    "        server.starttls()\n",
    "        server.login(smtp_user, smtp_password)\n",
    "        server.send_message(msg)\n",
    "        server.quit()\n",
    "        print(\"The email was sent successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: unable to send email: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create the notifier \"\"\"\n",
    "class Notifier8(BaseNotifier):\n",
    "    template_fields = (\"message\",)\n",
    "\n",
    "    def __init__(self, message):\n",
    "        self.message = message\n",
    "\n",
    "    def notify(self, context):\n",
    "        \"\"\" Send notification message \"\"\"\n",
    "        title = f\"Task {context['task_instance'].task_id} failed\"\n",
    "        send_my_message(title, self.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with DAG(\n",
    "    dag_id=\"example_notifier\",\n",
    "    start_date=datetime(2022, 1, 1),\n",
    "    schedule_interval=None,\n",
    "    tags=['notifier'],\n",
    "    on_success_callback=Notifier8(message=\"Success! Perfect!\"),\n",
    "    on_failure_callback=Notifier8(message=\"Failure! Oooops!\"),\n",
    ") as dag8:\n",
    "    task = BashOperator(\n",
    "        task_id=\"example_8_task\",\n",
    "        bash_command=\"exit 1\",\n",
    "        on_success_callback=Notifier8(message=\"Task Succeeded!\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DAG: example_notifier>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dag8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
